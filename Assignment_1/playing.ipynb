{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursive function for determinant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determ(A):\n",
    "    \n",
    "    A = np.array(A)\n",
    "    \n",
    "    assert (len(A.shape) == 2), \"Input matrix must be a 2D matrix: is {}D\".format(len(A.shape))\n",
    "    assert (A.shape[0] == A.shape[1]), \"Input matrix must be a square matrix: has {} rows and {} columns\".format(A.shape[0],A.shape[1])\n",
    "    \n",
    "    if A.shape == (1,1):\n",
    "        return A[0,0]\n",
    "    \n",
    "    else: \n",
    "        result = 0\n",
    "        for i in np.arange(A.shape[0]):\n",
    "            result += ((-1) ** i)*A[i, 0]*determ(np.delete(np.delete(A, i, 0), 0, 1))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a=np.array([[1,3],[3,2]])\n",
    "print(determ(a))\n",
    "\n",
    "a = np.array([[1,1,1],[2,2,2],[3,3,3]])\n",
    "print(determ(a))\n",
    "\n",
    "a = np.array([[1,2,3,4,5],[4,7,3,2,1],[5,7,0,1,3],[3,4,6,1,3],[0,4,7,1,2]])\n",
    "print(determ(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Stuff - because I am an expert at the NumPy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1/(1+np.exp(-np.array([1,2,3]))))\n",
    "\n",
    "import math\n",
    "[1/(1+math.exp(-y)) for y in np.array([1,2,3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 words, 3 embeddings\n",
    "U = np.array([[1,2,3,4,5],[5,4,2,6,7],[10,9,5,91,10]])\n",
    "v_c = np.array([1,2,3])\n",
    "outsideWordIdx = 1\n",
    "\n",
    "y = np.zeros(U.shape[1])\n",
    "y[outsideWordIdx] = 1\n",
    "print(y)\n",
    "\n",
    "# Compute 'y hat'\n",
    "u_o = U[:, outsideWordIdx]\n",
    "denominator = np.sum((U.T * v_c), axis=1)\n",
    "print(denominator)\n",
    "numerator = denominator[outsideWordIdx]\n",
    "print(numerator)\n",
    "y_hat = softmax(denominator)\n",
    "\n",
    "#print(denominator)\n",
    "print(y_hat)\n",
    "\n",
    "y_hat.shape = (5, 1)\n",
    "v_c.shape = (3, 1)\n",
    "print(v_c)\n",
    "\n",
    "print(v_c * y_hat.T)\n",
    "print('abc')\n",
    "\n",
    "print(y_hat[outsideWordIdx])\n",
    "print(y_hat[outsideWordIdx] * v_c - v_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.exp(41)/(np.exp(41)+np.exp(37)+np.exp(22)+np.exp(289)+np.exp(49)))\n",
    "print(np.exp(37)/(np.exp(41)+np.exp(37)+np.exp(22)+np.exp(289)+np.exp(49)))\n",
    "print(np.exp(22)/(np.exp(41)+np.exp(37)+np.exp(22)+np.exp(289)+np.exp(49)))\n",
    "print(np.exp(289)/(np.exp(41)+np.exp(37)+np.exp(22)+np.exp(289)+np.exp(49)))\n",
    "print(np.exp(49)/(np.exp(41)+np.exp(37)+np.exp(22)+np.exp(289)+np.exp(49)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((U.T * v_c).T)\n",
    "print(np.sum((U.T * v_c).T, axis=0))\n",
    "print(np.sum((U.T * v_c).T, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outsideWordIdx = 1\n",
    "\n",
    "y = np.zeros(U.shape[1])\n",
    "y[outsideWordIdx] = 1\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-np.log((U.T * v_c).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.zeros(U.shape[1])\n",
    "y[1] = 1\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute the softmax function for each row of the input x.\n",
    "    It is crucial that this function is optimized for speed because\n",
    "    it will be used frequently in later code. \n",
    "\n",
    "    Arguments:\n",
    "    x -- A D dimensional vector or N x D dimensional numpy matrix.\n",
    "    Return:\n",
    "    x -- You are allowed to modify x in-place\n",
    "    \"\"\"\n",
    "    orig_shape = x.shape\n",
    "\n",
    "    if len(x.shape) > 1:\n",
    "        # Matrix\n",
    "        tmp = np.max(x, axis=1)\n",
    "        x -= tmp.reshape((x.shape[0], 1))\n",
    "        x = np.exp(x)\n",
    "        tmp = np.sum(x, axis=1)\n",
    "        x /= tmp.reshape((x.shape[0], 1))\n",
    "    else:\n",
    "        # Vector\n",
    "        tmp = np.max(x)\n",
    "        x -= tmp\n",
    "        x = np.exp(x)\n",
    "        tmp = np.sum(x)\n",
    "        x /= tmp\n",
    "\n",
    "    assert x.shape == orig_shape\n",
    "    return x\n",
    "\n",
    "softmax(np.array([1,4,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 words, 3 embeddings\n",
    "U = np.array([[1,2,3,4,5],[5,4,2,6,7],[10,9,5,91,10]])\n",
    "v_c = np.array([1,2,3])\n",
    "outsideWordIdx = 1\n",
    "\n",
    "y = np.zeros(U.shape[1])\n",
    "y[outsideWordIdx] = 1\n",
    "\n",
    "print(U)\n",
    "\n",
    "print(v_c)\n",
    "\n",
    "u_o = U[:, outsideWordIdx]\n",
    "print(np.dot(u_o, v_c))\n",
    "\n",
    "u_k = U[:, [1,2,4]]\n",
    "\n",
    "print(u_k)\n",
    "print(np.sum(u_k.T * v_c, axis=1))\n",
    "\n",
    "\n",
    "print(sigmoid(np.sum(u_k.T * v_c, axis=1)))\n",
    "print(sigmoid(np.sum(u_k.T * v_c, axis=1)) - np.array([0.4999]))\n",
    "\n",
    "print(u_k * (sigmoid(np.sum(u_k.T * v_c, axis=1)) - np.array([0.4999])))\n",
    "\n",
    "print(np.sum(u_k * (sigmoid(np.sum(u_k.T * v_c, axis=1)) - np.array([0.4999])), axis = 1))\n",
    "\n",
    "print(U)\n",
    "print(v_c)\n",
    "print(np.sum(U.T * v_c, axis = 1))\n",
    "print(sigmoid(np.sum(U.T * v_c, axis = 1)) - np.array(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function for the input here.\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array.\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return s\n",
    "\n",
    "def normalizeRows(x):\n",
    "    \"\"\" Row normalization function\n",
    "\n",
    "    Implement a function that normalizes each row of a matrix to have\n",
    "    unit length.\n",
    "    \"\"\"\n",
    "    N = x.shape[0]\n",
    "    x /= np.sqrt(np.sum(x**2, axis=1)).reshape((N,1)) + 1e-30\n",
    "    return x\n",
    "\n",
    "import random\n",
    "\n",
    "dataset = type('dummy', (), {})()\n",
    "def dummySampleTokenIdx():\n",
    "    return random.randint(0, 4)\n",
    "\n",
    "def getRandomContext(C):\n",
    "    tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "    return tokens[random.randint(0,4)], \\\n",
    "        [tokens[random.randint(0,4)] for i in range(2*C)]\n",
    "dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "dataset.getRandomContext = getRandomContext\n",
    "\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "dummy_vectors = normalizeRows(np.random.randn(10,3))\n",
    "dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])\n",
    "\n",
    "\n",
    "currentCenterWord = \"c\"\n",
    "windowSize = 3\n",
    "outsideWords = [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"]\n",
    "word2Ind = dummy_tokens\n",
    "centerWordVectors = dummy_vectors[:5, :]\n",
    "outsideVectors = dummy_vectors[5:, :]\n",
    "dataset=dataset\n",
    "\n",
    "outsideWordIdx = word2Ind[currentCenterWord]\n",
    "centerWordVec = centerWordVectors[2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outsideVectors)\n",
    "print(\"\\n\")\n",
    "print(centerWordVectors)\n",
    "print(\"\\n\")\n",
    "print(centerWordVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denominator = np.dot(outsideVectors, centerWordVec)\n",
    "print(denominator)\n",
    "\n",
    "y_hat = softmax(denominator)\n",
    "print(y_hat)\n",
    "\n",
    "y = np.zeros(5)\n",
    "y[2] = 1\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_hat)\n",
    "print('\\n')\n",
    "print(centerWordVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quad = lambda x: (np.sum(x ** 2), x * 2)\n",
    "quad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quad(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat[2] * centerWordVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat1 = np.array(y_hat)\n",
    "y_hat1.shape = (y_hat1.shape[0], 1)\n",
    "\n",
    "y1 = np.array(y)\n",
    "y1.shape = (y1.shape[0], 1)\n",
    "\n",
    "print((y_hat1-y1).shape)\n",
    "\n",
    "centerWordVec1 = np.array(centerWordVec)\n",
    "centerWordVec1.shape = (centerWordVec1.shape[0], 1)\n",
    "\n",
    "print(centerWordVec1.shape)\n",
    "\n",
    "print(centerWordVec1.T * (y_hat1-y1))\n",
    "\n",
    "print(np.dot((y_hat-y)[:, None], centerWordVec[None, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outsideVectors)\n",
    "print(\"\\n\")\n",
    "print(centerWordVectors)\n",
    "print(\"\\n\")\n",
    "print(centerWordVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_o = outsideVectors[2, :]\n",
    "print(u_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(u_o, centerWordVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_k = outsideVectors[ [1,2], :]\n",
    "print(u_k)\n",
    "\n",
    "u_o * (sigmoid(np.dot(u_o, centerWordVec))-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(np.dot(u_k, centerWordVec)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_k.T * (sigmoid(np.dot(u_k, centerWordVec)) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(u_k.T * (sigmoid(np.dot(u_k, centerWordVec)) - 1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sigmoid(np.dot(u_o, centerWordVec)) - 1) * centerWordVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1=sigmoid(np.dot(u_k, centerWordVec)) - 1\n",
    "a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centerWordVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(a1[:, None], centerWordVec[None, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-0.63372281 * centerWordVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, windowsize = 5, naiveSoftmaxLossAndGradient),\n",
    "        dummy_vectors, \"naiveSoftmaxLossAndGradient Gradient\")\n",
    "\n",
    "windowSize = 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(-3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.sampleTokenIdx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=10\n",
    "negSampleWordIndices = [None] * K\n",
    "for k in range(K):\n",
    "    newidx = dataset.sampleTokenIdx()\n",
    "    while newidx == outsideWordIdx:\n",
    "        newidx = dataset.sampleTokenIdx()\n",
    "    negSampleWordIndices[k] = newidx\n",
    "\n",
    "negSampleWordIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [1] + negSampleWordIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outsideVectors[negSampleWordIndices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=10\n",
    "negSampleWordIndices = [None] * K\n",
    "for k in range(K):\n",
    "    newidx = dataset.sampleTokenIdx()\n",
    "    while newidx == outsideWordIdx:\n",
    "        newidx = dataset.sampleTokenIdx()\n",
    "    negSampleWordIndices[k] = newidx\n",
    "\n",
    "indices = [1] + negSampleWordIndices\n",
    "print(\"indices: {}\".format(indices))\n",
    "print(\"\\n\")\n",
    "\n",
    "negOutsideVecs = outsideVectors[indices, :] #u_k\n",
    "print(\"negOutsideVecs: {}\".format(negOutsideVecs))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Loss\n",
    "print(\"sigmoid(np.dot(negOutsideVecs, centerWordVec)): {}\".format(sigmoid(np.dot(negOutsideVecs, centerWordVec))))\n",
    "print(\"\\n\")\n",
    "print(\"centerWordVec: {}\".format(centerWordVec))\n",
    "\n",
    "loss = sigmoid(np.dot(u_o, centerWordVec))+np.sum(sigmoid(np.dot(negOutsideVecs, centerWordVec)))\n",
    "print(\"Loss: {}\".format(loss))\n",
    "print(\"\\n\")\n",
    "\n",
    "# dJ / dv_c\n",
    "gradCenterVec = u_o * (sigmoid(np.dot(u_o, centerWordVec))-1) - np.sum(negOutsideVecs.T * (sigmoid(np.dot(negOutsideVecs, centerWordVec)) - 1), axis=1)\n",
    "print(\"gradCenterVec: {}\".format(gradCenterVec))\n",
    "print(\"\\n\")\n",
    "\n",
    "# dJ / du_k\n",
    "gradOutsideVecs = np.zeros(outsideVectors.shape)\n",
    "\n",
    "dJ_du_o = (sigmoid(np.dot(u_o, centerWordVec)) - 1) * centerWordVec\n",
    "\n",
    "dsigmoid_u = sigmoid(np.dot(negOutsideVecs, centerWordVec)) - 1\n",
    "dJ_du_k = np.dot(dsigmoid_u[:, None], centerWordVec[None, :])\n",
    "\n",
    "gradOutsideVecs[outsideWordIdx, :] = dJ_du_o\n",
    "gradOutsideVecs[indices, :] = dJ_du_k\n",
    "print(\"gradOutsideVecs: {}\".format(gradOutsideVecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outsideWordIdx = 2\n",
    "centerWordIdx = 2\n",
    "\n",
    "print(\"Outside Vectors: \\n {}\".format(outsideVectors))\n",
    "print(\"Center Vectors: \\n {}\".format(centerWordVectors))\n",
    "\n",
    "\n",
    "K=10\n",
    "negSampleWordIndices = [None] * K\n",
    "for k in range(K):\n",
    "    newidx = dataset.sampleTokenIdx()\n",
    "    while newidx == outsideWordIdx:\n",
    "        newidx = dataset.sampleTokenIdx()\n",
    "    negSampleWordIndices[k] = newidx\n",
    "    \n",
    "indices = negSampleWordIndices\n",
    "centerWordVec = centerWordVectors[centerWordIdx, :]\n",
    "\n",
    "print(\"Sampled indices: \\n {}\".format(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_o = outsideVectors[outsideWordIdx, :]\n",
    "print(u_o)\n",
    "\n",
    "u_k_matrix = outsideVectors[indices, :]\n",
    "print(u_k_matrix)\n",
    "\n",
    "print(centerWordVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_o*(1-sigmoid(np.dot(centerWordVec, u_o)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = 1-sigmoid(-np.dot(u_k_matrix, centerWordVec))\n",
    "a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_k_matrix * a1[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(u_k_matrix * a1[:, None], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.log(sigmoid(-np.dot(u_k_matrix, centerWordVec))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Indices: \\n{}\".format(indices))\n",
    "print(\"Outside Vectors: \\n{}\".format(outsideVectors))\n",
    "print(\"Center Vector: \\n{}\".format(centerWordVec))\n",
    "print(\"U_o: \\n{}\".format(u_o))\n",
    "print(\"U_k matrix: \\n{}\".format(u_k_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1-sigmoid(np.dot(u_o, centerWordVec)))*centerWordVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(10):\n",
    "    print((sigmoid(-np.dot(u_k_matrix[k, :], centerWordVec))-1)*centerWordVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    orig_shape = x.shape\n",
    "\n",
    "    if len(x.shape) > 1:\n",
    "        # Matrix\n",
    "        tmp = np.max(x, axis=1)\n",
    "        x -= tmp.reshape((x.shape[0], 1))\n",
    "        x = np.exp(x)\n",
    "        tmp = np.sum(x, axis=1)\n",
    "        x /= tmp.reshape((x.shape[0], 1))\n",
    "    else:\n",
    "        # Vector\n",
    "        tmp = np.max(x)\n",
    "        x -= tmp\n",
    "        x = np.exp(x)\n",
    "        tmp = np.sum(x)\n",
    "        x /= tmp\n",
    "\n",
    "    assert x.shape == orig_shape\n",
    "    return x\n",
    "\n",
    "def naiveSoftmaxLossAndGradient(\n",
    "    centerWordVec,\n",
    "    outsideWordIdx,\n",
    "    outsideVectors,\n",
    "    dataset\n",
    "):\n",
    "\n",
    "    # Compute 'y'\n",
    "    num_words = outsideVectors.shape[0]\n",
    "\n",
    "    y = np.zeros(num_words)\n",
    "    y[outsideWordIdx] = 1\n",
    "\n",
    "    # Compute 'y hat' by getting the U transpose * v_c for every outside word\n",
    "    denominator = np.dot(outsideVectors, centerWordVec)\n",
    "    y_hat = softmax(denominator)\n",
    "\n",
    "    # Calculate loss for the mentioned outside word\n",
    "    loss = -np.log(y_hat[outsideWordIdx])\n",
    "\n",
    "    # Calculate Gradient wrt center word\n",
    "    gradCenterVec = np.dot(outsideVectors.T, y-y_hat)\n",
    "\n",
    "    # Calculate Gradient wrt all outside word\n",
    "    gradOutsideVecs = np.dot((y-y_hat)[:, None], centerWordVec[None, :])\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return loss, gradCenterVec, gradOutsideVecs\n",
    "\n",
    "def getNegativeSamples(outsideWordIdx, dataset, K):\n",
    "    \"\"\" Samples K indexes which are not the outsideWordIdx \"\"\"\n",
    "\n",
    "    negSampleWordIndices = [None] * K\n",
    "    for k in range(K):\n",
    "        newidx = dataset.sampleTokenIdx()\n",
    "        while newidx == outsideWordIdx:\n",
    "            newidx = dataset.sampleTokenIdx()\n",
    "        negSampleWordIndices[k] = newidx\n",
    "    return negSampleWordIndices\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \n",
    "    s = 1/(1+np.exp(-x))\n",
    "\n",
    "    return s\n",
    "\n",
    "def normalizeRows(x):\n",
    "    N = x.shape[0]\n",
    "    x /= np.sqrt(np.sum(x**2, axis=1)).reshape((N,1)) + 1e-30\n",
    "    return x\n",
    "\n",
    "# First implement a gradient checker by filling in the following functions\n",
    "def word2vec_sgd_wrapper(word2vecModel, word2Ind, wordVectors, dataset,\n",
    "                         windowSize,\n",
    "                         word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
    "    batchsize = 50\n",
    "    loss = 0.0\n",
    "    grad = np.zeros(wordVectors.shape)\n",
    "    N = wordVectors.shape[0]\n",
    "    centerWordVectors = wordVectors[:int(N/2),:]\n",
    "    outsideVectors = wordVectors[int(N/2):,:]\n",
    "    for i in range(batchsize):\n",
    "        windowSize1 = random.randint(1, windowSize)\n",
    "        centerWord, context = dataset.getRandomContext(windowSize1)\n",
    "\n",
    "        c, gin, gout = word2vecModel(\n",
    "            centerWord, windowSize1, context, word2Ind, centerWordVectors,\n",
    "            outsideVectors, dataset, word2vecLossAndGradient\n",
    "        )\n",
    "        loss += c / batchsize\n",
    "        grad[:int(N/2), :] += gin / batchsize\n",
    "        grad[int(N/2):, :] += gout / batchsize\n",
    "\n",
    "    return loss, grad\n",
    "\n",
    "def gradcheck_naive(f, x, gradientText):\n",
    "    \"\"\" Gradient check for a function f.\n",
    "    Arguments:\n",
    "    f -- a function that takes a single argument and outputs the\n",
    "         loss and its gradients\n",
    "    x -- the point (numpy array) to check the gradient at\n",
    "    gradientText -- a string detailing some context about the gradient computation\n",
    "    \"\"\"\n",
    "\n",
    "    rndstate = random.getstate()\n",
    "    random.setstate(rndstate)\n",
    "    fx, grad = f(x) # Evaluate function value at original point\n",
    "    h = 1e-4        # Do not change this!\n",
    "\n",
    "    # Iterate over all indexes ix in x to check the gradient.\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        x[ix] += h # increment by h\n",
    "        random.setstate(rndstate)\n",
    "        fxh, _ = f(x) # evalute f(x + h)\n",
    "        x[ix] -= 2 * h # restore to previous value (very important!)\n",
    "        random.setstate(rndstate)\n",
    "        fxnh, _ = f(x)\n",
    "        x[ix] += h\n",
    "        numgrad = (fxh - fxnh) / 2 / h\n",
    "\n",
    "        # Compare gradients\n",
    "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
    "        if reldiff > 1e-5:\n",
    "            print(\"Gradient check failed for %s.\" % gradientText)\n",
    "            print(\"First gradient error found at index %s in the vector of gradients\" % str(ix))\n",
    "            print(\"Your gradient: %f \\t Numerical gradient: %f\" % (\n",
    "                grad[ix], numgrad))\n",
    "            return\n",
    "        else:\n",
    "            print(\"Numerical Gradient: {}, Your Gradient: {}\".format(numgrad, grad[ix]))\n",
    "\n",
    "        it.iternext() # Step to next dimension\n",
    "\n",
    "    print(\"Gradient check passed!\")\n",
    "\n",
    "def skipgram(currentCenterWord, windowSize, outsideWords, word2Ind,\n",
    "             centerWordVectors, outsideVectors, dataset,\n",
    "             word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
    "\n",
    "    loss = 0.0\n",
    "    gradCenterVecs = np.zeros(centerWordVectors.shape)\n",
    "    gradOutsideVectors = np.zeros(outsideVectors.shape)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    centerWordIdx = word2Ind[currentCenterWord]\n",
    "    centerWordVec = centerWordVectors[centerWordIdx, :]\n",
    "\n",
    "    for j in np.arange(-windowSize, windowSize+1):\n",
    "        if j != 0 and centerWordIdx + j < outsideVectors.shape[0]:\n",
    "            outsideWordIdx = centerWordIdx + j\n",
    "            temp_loss,temp_gradCenterVecs,temp_gradOutsideVecs = word2vecLossAndGradient(\n",
    "            centerWordVec = centerWordVec,\n",
    "            outsideWordIdx = outsideWordIdx,\n",
    "            outsideVectors = outsideVectors,\n",
    "            dataset = dataset)\n",
    "\n",
    "            loss -= temp_loss\n",
    "            gradCenterVecs[centerWordIdx, :] += temp_gradCenterVecs\n",
    "            gradOutsideVectors += temp_gradOutsideVecs\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return loss, gradCenterVecs, gradOutsideVectors\n",
    "    \n",
    "import random\n",
    "\n",
    "dataset = type('dummy', (), {})()\n",
    "def dummySampleTokenIdx():\n",
    "    return random.randint(0, 4)\n",
    "\n",
    "def getRandomContext(C):\n",
    "    tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "    return tokens[random.randint(0,4)], \\\n",
    "        [tokens[random.randint(0,4)] for i in range(2*C)]\n",
    "dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "dataset.getRandomContext = getRandomContext\n",
    "\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "dummy_vectors = normalizeRows(np.random.randn(10,3))\n",
    "dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])\n",
    "\n",
    "\n",
    "def negSamplingLossAndGradient(\n",
    "    centerWordVec,\n",
    "    outsideWordIdx,\n",
    "    outsideVectors,\n",
    "    dataset,\n",
    "    K=10\n",
    "):\n",
    "\n",
    "    # Negative sampling of words is done for you. Do not modify this if you\n",
    "    # wish to match the autograder and receive points!\n",
    "    negSampleWordIndices = getNegativeSamples(outsideWordIdx, dataset, K)\n",
    "    indices = [outsideWordIdx] + negSampleWordIndices\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### Please use your implementation of sigmoid in here.\n",
    "    u_o = outsideVectors[outsideWordIdx, :]\n",
    "\n",
    "    negOutsideVecs = -outsideVectors[negSampleWordIndices, :]\n",
    "    negOutsideVecs = np.append([u_o], negOutsideVecs, axis=0)\n",
    "\n",
    "    # Loss\n",
    "    loss = np.sum(np.log(sigmoid(np.dot(negOutsideVecs, centerWordVec))))\n",
    "\n",
    "    # dJ / dv_c\n",
    "    gradCenter_t1 = sigmoid(np.dot(negOutsideVecs, centerWordVec))-1\n",
    "    gradCenterVec = np.sum(negOutsideVecs*gradCenter_t1[:,None], axis=0)\n",
    "\n",
    "    # dJ / du_k\n",
    "    gradOutsideVecs = np.zeros(outsideVectors.shape)\n",
    "\n",
    "    gradOutsideVecs[outsideWordIdx, :] = centerWordVec * (sigmoid(np.dot(centerWordVec, u_o))-1)\n",
    "\n",
    "    dJ_du_k = centerWordVec * (1- sigmoid(np.dot(negOutsideVecs[1:,:], centerWordVec[:, None])))\n",
    "\n",
    "    for k in range(len(negSampleWordIndices)):\n",
    "        gradOutsideVecs[negSampleWordIndices[k], :] += dJ_du_k[k, :]\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return loss, gradCenterVec, gradOutsideVecs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Gradient check for skip-gram with negSamplingLossAndGradient ====\n",
      "Numerical Gradient: -1.8363585684255668, Your Gradient: -1.8363585685686517\n",
      "Numerical Gradient: -0.754368030690955, Your Gradient: -0.7543680308709707\n",
      "Numerical Gradient: -0.007461267870212396, Your Gradient: -0.007461267771052713\n",
      "Numerical Gradient: -1.6898269105070085, Your Gradient: -1.6898269103340589\n",
      "Numerical Gradient: -0.2025758913148934, Your Gradient: -0.20257589137640525\n",
      "Numerical Gradient: -1.6931543788700765, Your Gradient: -1.6931543789960457\n",
      "Numerical Gradient: -2.591218162066866, Your Gradient: -2.5912181621152266\n",
      "Numerical Gradient: -0.6004862125763566, Your Gradient: -0.600486212665055\n",
      "Numerical Gradient: 0.27371866782033294, Your Gradient: 0.27371866812410833\n",
      "Numerical Gradient: -1.7634945014322057, Your Gradient: -1.763494501480897\n",
      "Numerical Gradient: -0.6652648108840253, Your Gradient: -0.6652648110180548\n",
      "Numerical Gradient: -1.0525870243327518, Your Gradient: -1.0525870244098097\n",
      "Numerical Gradient: -0.7090057914638237, Your Gradient: -0.7090057914673225\n",
      "Numerical Gradient: 0.14273182667778883, Your Gradient: 0.14273182673035184\n",
      "Numerical Gradient: -0.02539616339447548, Your Gradient: -0.025396163408965736\n",
      "Numerical Gradient: -1.7504451991356973, Your Gradient: -1.7504451994688317\n",
      "Numerical Gradient: 0.21070988942994973, Your Gradient: 0.21070988950564626\n",
      "Numerical Gradient: 1.2214265097298949, Your Gradient: 1.2214265102040291\n",
      "Numerical Gradient: -0.021054912409113058, Your Gradient: -0.021054912368804222\n",
      "Numerical Gradient: -0.7286710413367814, Your Gradient: -0.7286710412586335\n",
      "Numerical Gradient: -0.8322736932342423, Your Gradient: -0.8322736934827947\n",
      "Numerical Gradient: -0.9999854089670634, Your Gradient: -0.999985409271216\n",
      "Numerical Gradient: -0.3336091362626803, Your Gradient: -0.3336091361247987\n",
      "Numerical Gradient: 0.10526658577703074, Your Gradient: 0.1052665855214059\n",
      "Numerical Gradient: -0.7290067471643624, Your Gradient: -0.729006747151703\n",
      "Numerical Gradient: -0.9257227213410602, Your Gradient: -0.9257227214187949\n",
      "Numerical Gradient: -0.9079650089560687, Your Gradient: -0.9079650094309748\n",
      "Numerical Gradient: -1.1559060963861612, Your Gradient: -1.155906096701062\n",
      "Numerical Gradient: -0.4044372889211445, Your Gradient: -0.4044372888520631\n",
      "Numerical Gradient: 0.29012062693567486, Your Gradient: 0.29012062714177533\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "print(\"==== Gradient check for skip-gram with negSamplingLossAndGradient ====\")\n",
    "gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "    skipgram, dummy_tokens, vec, dataset, 5, negSamplingLossAndGradient),\n",
    "    dummy_vectors, \"negSamplingLossAndGradient Gradient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
