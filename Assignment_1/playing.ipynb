{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursive function for determinant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determ(A):\n",
    "    \n",
    "    A = np.array(A)\n",
    "    \n",
    "    assert (len(A.shape) == 2), \"Input matrix must be a 2D matrix: is {}D\".format(len(A.shape))\n",
    "    assert (A.shape[0] == A.shape[1]), \"Input matrix must be a square matrix: has {} rows and {} columns\".format(A.shape[0],A.shape[1])\n",
    "    \n",
    "    if A.shape == (1,1):\n",
    "        return A[0,0]\n",
    "    \n",
    "    else: \n",
    "        result = 0\n",
    "        for i in np.arange(A.shape[0]):\n",
    "            result += ((-1) ** i)*A[i, 0]*determ(np.delete(np.delete(A, i, 0), 0, 1))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7\n",
      "0\n",
      "1368\n"
     ]
    }
   ],
   "source": [
    "a=np.array([[1,3],[3,2]])\n",
    "print(determ(a))\n",
    "\n",
    "a = np.array([[1,1,1],[2,2,2],[3,3,3]])\n",
    "print(determ(a))\n",
    "\n",
    "a = np.array([[1,2,3,4,5],[4,7,3,2,1],[5,7,0,1,3],[3,4,6,1,3],[0,4,7,1,2]])\n",
    "print(determ(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Stuff - because I am an expert at the NumPy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73105858 0.88079708 0.95257413]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7310585786300049, 0.8807970779778823, 0.9525741268224334]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(1/(1+np.exp(-np.array([1,2,3]))))\n",
    "\n",
    "import math\n",
    "[1/(1+math.exp(-y)) for y in np.array([1,2,3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0. 0.]\n",
      "[ 41  37  22 289  49]\n",
      "37\n",
      "[1.97227962e-108 3.61235614e-110 1.10502813e-116 1.00000000e+000\n",
      " 5.87928270e-105]\n",
      "[[1]\n",
      " [2]\n",
      " [3]]\n",
      "[[1.97227962e-108 3.61235614e-110 1.10502813e-116 1.00000000e+000\n",
      "  5.87928270e-105]\n",
      " [3.94455925e-108 7.22471228e-110 2.21005625e-116 2.00000000e+000\n",
      "  1.17585654e-104]\n",
      " [5.91683887e-108 1.08370684e-109 3.31508438e-116 3.00000000e+000\n",
      "  1.76378481e-104]]\n",
      "abc\n",
      "[3.61235614e-110]\n",
      "[[-1.]\n",
      " [-2.]\n",
      " [-3.]]\n"
     ]
    }
   ],
   "source": [
    "# 5 words, 3 embeddings\n",
    "U = np.array([[1,2,3,4,5],[5,4,2,6,7],[10,9,5,91,10]])\n",
    "v_c = np.array([1,2,3])\n",
    "outsideWordIdx = 1\n",
    "\n",
    "y = np.zeros(U.shape[1])\n",
    "y[outsideWordIdx] = 1\n",
    "print(y)\n",
    "\n",
    "# Compute 'y hat'\n",
    "u_o = U[:, outsideWordIdx]\n",
    "denominator = np.sum((U.T * v_c), axis=1)\n",
    "print(denominator)\n",
    "numerator = denominator[outsideWordIdx]\n",
    "print(numerator)\n",
    "y_hat = softmax(denominator)\n",
    "\n",
    "#print(denominator)\n",
    "print(y_hat)\n",
    "\n",
    "y_hat.shape = (5, 1)\n",
    "v_c.shape = (3, 1)\n",
    "print(v_c)\n",
    "\n",
    "print(v_c * y_hat.T)\n",
    "print('abc')\n",
    "\n",
    "print(y_hat[outsideWordIdx])\n",
    "print(y_hat[outsideWordIdx] * v_c - v_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9722796241351285e-108\n",
      "3.6123561383267394e-110\n",
      "1.105028125193164e-116\n",
      "1.0\n",
      "5.8792826982452694e-105\n"
     ]
    }
   ],
   "source": [
    "print(np.exp(41)/(np.exp(41)+np.exp(37)+np.exp(22)+np.exp(289)+np.exp(49)))\n",
    "print(np.exp(37)/(np.exp(41)+np.exp(37)+np.exp(22)+np.exp(289)+np.exp(49)))\n",
    "print(np.exp(22)/(np.exp(41)+np.exp(37)+np.exp(22)+np.exp(289)+np.exp(49)))\n",
    "print(np.exp(289)/(np.exp(41)+np.exp(37)+np.exp(22)+np.exp(289)+np.exp(49)))\n",
    "print(np.exp(49)/(np.exp(41)+np.exp(37)+np.exp(22)+np.exp(289)+np.exp(49)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1   2   3   4   5]\n",
      " [ 10   8   4  12  14]\n",
      " [ 30  27  15 273  30]]\n",
      "[ 41  37  22 289  49]\n",
      "[ 15  48 375]\n"
     ]
    }
   ],
   "source": [
    "print((U.T * v_c).T)\n",
    "print(np.sum((U.T * v_c).T, axis=0))\n",
    "print(np.sum((U.T * v_c).T, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outsideWordIdx = 1\n",
    "\n",
    "y = np.zeros(U.shape[1])\n",
    "y[outsideWordIdx] = 1\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.        , -0.69314718, -1.09861229, -1.38629436, -1.60943791],\n",
       "       [-2.30258509, -2.07944154, -1.38629436, -2.48490665, -2.63905733],\n",
       "       [-3.40119738, -3.29583687, -2.7080502 , -5.6094718 , -3.40119738]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log((U.T * v_c).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=np.zeros(U.shape[1])\n",
    "y[1] = 1\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 2., 0., 0., 0.],\n",
       "       [0., 4., 0., 0., 0.],\n",
       "       [0., 9., 0., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04201007, 0.84379473, 0.1141952 ])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute the softmax function for each row of the input x.\n",
    "    It is crucial that this function is optimized for speed because\n",
    "    it will be used frequently in later code. \n",
    "\n",
    "    Arguments:\n",
    "    x -- A D dimensional vector or N x D dimensional numpy matrix.\n",
    "    Return:\n",
    "    x -- You are allowed to modify x in-place\n",
    "    \"\"\"\n",
    "    orig_shape = x.shape\n",
    "\n",
    "    if len(x.shape) > 1:\n",
    "        # Matrix\n",
    "        tmp = np.max(x, axis=1)\n",
    "        x -= tmp.reshape((x.shape[0], 1))\n",
    "        x = np.exp(x)\n",
    "        tmp = np.sum(x, axis=1)\n",
    "        x /= tmp.reshape((x.shape[0], 1))\n",
    "    else:\n",
    "        # Vector\n",
    "        tmp = np.max(x)\n",
    "        x -= tmp\n",
    "        x = np.exp(x)\n",
    "        tmp = np.sum(x)\n",
    "        x /= tmp\n",
    "\n",
    "    assert x.shape == orig_shape\n",
    "    return x\n",
    "\n",
    "softmax(np.array([1,4,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4  5]\n",
      " [ 5  4  2  6  7]\n",
      " [10  9  5 91 10]]\n",
      "[1 2 3]\n",
      "37\n",
      "[[ 2  3  5]\n",
      " [ 4  2  7]\n",
      " [ 9  5 10]]\n",
      "[37 22 49]\n",
      "[1.0, 0.9999999997210531, 1.0]\n",
      "[0.5001 0.5001 0.5001]\n",
      "[[1.0002 1.5003 2.5005]\n",
      " [2.0004 1.0002 3.5007]\n",
      " [4.5009 2.5005 5.001 ]]\n",
      "[ 5.001   6.5013 12.0024]\n",
      "[[ 1  2  3  4  5]\n",
      " [ 5  4  2  6  7]\n",
      " [10  9  5 91 10]]\n",
      "[1 2 3]\n",
      "[ 41  37  22 289  49]\n",
      "[ 0.00000000e+00  0.00000000e+00 -2.78946866e-10  0.00000000e+00\n",
      "  0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# 5 words, 3 embeddings\n",
    "U = np.array([[1,2,3,4,5],[5,4,2,6,7],[10,9,5,91,10]])\n",
    "v_c = np.array([1,2,3])\n",
    "outsideWordIdx = 1\n",
    "\n",
    "y = np.zeros(U.shape[1])\n",
    "y[outsideWordIdx] = 1\n",
    "\n",
    "print(U)\n",
    "\n",
    "print(v_c)\n",
    "\n",
    "u_o = U[:, outsideWordIdx]\n",
    "print(np.dot(u_o, v_c))\n",
    "\n",
    "u_k = U[:, [1,2,4]]\n",
    "\n",
    "print(u_k)\n",
    "print(np.sum(u_k.T * v_c, axis=1))\n",
    "\n",
    "\n",
    "print(sigmoid(np.sum(u_k.T * v_c, axis=1)))\n",
    "print(sigmoid(np.sum(u_k.T * v_c, axis=1)) - np.array([0.4999]))\n",
    "\n",
    "print(u_k * (sigmoid(np.sum(u_k.T * v_c, axis=1)) - np.array([0.4999])))\n",
    "\n",
    "print(np.sum(u_k * (sigmoid(np.sum(u_k.T * v_c, axis=1)) - np.array([0.4999])), axis = 1))\n",
    "\n",
    "print(U)\n",
    "print(v_c)\n",
    "print(np.sum(U.T * v_c, axis = 1))\n",
    "print(sigmoid(np.sum(U.T * v_c, axis = 1)) - np.array(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function for the input here.\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array.\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return s\n",
    "\n",
    "def normalizeRows(x):\n",
    "    \"\"\" Row normalization function\n",
    "\n",
    "    Implement a function that normalizes each row of a matrix to have\n",
    "    unit length.\n",
    "    \"\"\"\n",
    "    N = x.shape[0]\n",
    "    x /= np.sqrt(np.sum(x**2, axis=1)).reshape((N,1)) + 1e-30\n",
    "    return x\n",
    "\n",
    "import random\n",
    "\n",
    "dataset = type('dummy', (), {})()\n",
    "def dummySampleTokenIdx():\n",
    "    return random.randint(0, 4)\n",
    "\n",
    "def getRandomContext(C):\n",
    "    tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "    return tokens[random.randint(0,4)], \\\n",
    "        [tokens[random.randint(0,4)] for i in range(2*C)]\n",
    "dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "dataset.getRandomContext = getRandomContext\n",
    "\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "dummy_vectors = normalizeRows(np.random.randn(10,3))\n",
    "dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])\n",
    "\n",
    "\n",
    "currentCenterWord = \"c\"\n",
    "windowSize = 3\n",
    "outsideWords = [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"]\n",
    "word2Ind = dummy_tokens\n",
    "centerWordVectors = dummy_vectors[:5, :]\n",
    "outsideVectors = dummy_vectors[5:, :]\n",
    "dataset=dataset\n",
    "\n",
    "outsideWordIdx = word2Ind[currentCenterWord]\n",
    "centerWordVec = centerWordVectors[2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.6831809  -0.04200519  0.72904007]\n",
      " [ 0.18289107  0.76098587 -0.62245591]\n",
      " [-0.61517874  0.5147624  -0.59713884]\n",
      " [-0.33867074 -0.80966534 -0.47931635]\n",
      " [-0.52629529 -0.78190408  0.33412466]]\n",
      "\n",
      "\n",
      "[[-0.96735714 -0.02182641  0.25247529]\n",
      " [ 0.73663029 -0.48088687 -0.47552459]\n",
      " [-0.27323645  0.12538062  0.95374082]\n",
      " [-0.56713774 -0.27178229 -0.77748902]\n",
      " [-0.59609459  0.7795666   0.19221644]]\n",
      "\n",
      "\n",
      "[-0.27323645  0.12538062  0.95374082]\n"
     ]
    }
   ],
   "source": [
    "print(outsideVectors)\n",
    "print(\"\\n\")\n",
    "print(centerWordVectors)\n",
    "print(\"\\n\")\n",
    "print(centerWordVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.87671855 -0.54822123 -0.33688521 -0.46612273  0.36443576]\n",
      "[0.41703564 0.10030664 0.12391154 0.10888915 0.24985703]\n",
      "[0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "denominator = np.dot(outsideVectors, centerWordVec)\n",
    "print(denominator)\n",
    "\n",
    "y_hat = softmax(denominator)\n",
    "print(y_hat)\n",
    "\n",
    "y = np.zeros(5)\n",
    "y[2] = 1\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.41703564 0.10030664 0.12391154 0.10888915 0.24985703]\n",
      "\n",
      "\n",
      "[-0.27323645  0.12538062  0.95374082]\n"
     ]
    }
   ],
   "source": [
    "print(y_hat)\n",
    "print('\\n')\n",
    "print(centerWordVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quad = lambda x: (np.sum(x ** 2), x * 2)\n",
    "quad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quad(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03385715,  0.01553611,  0.11817949])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat[2] * centerWordVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1)\n",
      "(3, 1)\n",
      "[[-0.11394933  0.05228819  0.39774391]\n",
      " [-0.02740743  0.01257651  0.09566654]\n",
      " [ 0.2393793  -0.10984452 -0.83556132]\n",
      " [-0.02975248  0.01365259  0.10385203]\n",
      " [-0.06827005  0.03132723  0.23829885]]\n",
      "[[-0.11394933  0.05228819  0.39774391]\n",
      " [-0.02740743  0.01257651  0.09566654]\n",
      " [ 0.2393793  -0.10984452 -0.83556132]\n",
      " [-0.02975248  0.01365259  0.10385203]\n",
      " [-0.06827005  0.03132723  0.23829885]]\n"
     ]
    }
   ],
   "source": [
    "y_hat1 = np.array(y_hat)\n",
    "y_hat1.shape = (y_hat1.shape[0], 1)\n",
    "\n",
    "y1 = np.array(y)\n",
    "y1.shape = (y1.shape[0], 1)\n",
    "\n",
    "print((y_hat1-y1).shape)\n",
    "\n",
    "centerWordVec1 = np.array(centerWordVec)\n",
    "centerWordVec1.shape = (centerWordVec1.shape[0], 1)\n",
    "\n",
    "print(centerWordVec1.shape)\n",
    "\n",
    "print(centerWordVec1.T * (y_hat1-y1))\n",
    "\n",
    "print(np.dot((y_hat-y)[:, None], centerWordVec[None, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.6831809  -0.04200519  0.72904007]\n",
      " [ 0.18289107  0.76098587 -0.62245591]\n",
      " [-0.61517874  0.5147624  -0.59713884]\n",
      " [-0.33867074 -0.80966534 -0.47931635]\n",
      " [-0.52629529 -0.78190408  0.33412466]]\n",
      "\n",
      "\n",
      "[[-0.96735714 -0.02182641  0.25247529]\n",
      " [ 0.73663029 -0.48088687 -0.47552459]\n",
      " [-0.27323645  0.12538062  0.95374082]\n",
      " [-0.56713774 -0.27178229 -0.77748902]\n",
      " [-0.59609459  0.7795666   0.19221644]]\n",
      "\n",
      "\n",
      "[-0.27323645  0.12538062  0.95374082]\n"
     ]
    }
   ],
   "source": [
    "print(outsideVectors)\n",
    "print(\"\\n\")\n",
    "print(centerWordVectors)\n",
    "print(\"\\n\")\n",
    "print(centerWordVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.61517874  0.5147624  -0.59713884]\n"
     ]
    }
   ],
   "source": [
    "u_o = outsideVectors[2, :]\n",
    "print(u_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.33688520543359074"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(u_o, centerWordVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.18289107  0.76098587 -0.62245591]\n",
      " [-0.61517874  0.5147624  -0.59713884]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.35891601, -0.30032973,  0.34839093])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_k = outsideVectors[ [1,2], :]\n",
    "print(u_k)\n",
    "\n",
    "u_o * (sigmoid(np.dot(u_o, centerWordVec))-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.63372281, -0.5834337 ])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.dot(u_k, centerWordVec)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11590224,  0.35891601],\n",
       "       [-0.4822541 , -0.30032973],\n",
       "       [ 0.39446451,  0.34839093]])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_k.T * (sigmoid(np.dot(u_k, centerWordVec)) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.24301377, -0.78258383,  0.74285543])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(u_k.T * (sigmoid(np.dot(u_k, centerWordVec)) - 1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.15941535, -0.07315128, -0.55644454])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sigmoid(np.dot(u_o, centerWordVec)) - 1) * centerWordVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.63372281, -0.5834337 ])"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1=sigmoid(np.dot(u_k, centerWordVec)) - 1\n",
    "a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.27323645,  0.12538062,  0.95374082])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centerWordVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.17315617, -0.07945656, -0.60440731],\n",
       "       [ 0.15941535, -0.07315128, -0.55644454]])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(a1[:, None], centerWordVec[None, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.17315617, -0.07945656, -0.60440731])"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-0.63372281 * centerWordVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, windowsize = 5, naiveSoftmaxLossAndGradient),\n",
    "        dummy_vectors, \"naiveSoftmaxLossAndGradient Gradient\")\n",
    "\n",
    "windowSize = 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.96735714, -0.02182641,  0.25247529],\n",
       "       [ 0.73663029, -0.48088687, -0.47552459],\n",
       "       [-0.27323645,  0.12538062,  0.95374082],\n",
       "       [-0.56713774, -0.27178229, -0.77748902],\n",
       "       [-0.59609459,  0.7795666 ,  0.19221644],\n",
       "       [-0.6831809 , -0.04200519,  0.72904007],\n",
       "       [ 0.18289107,  0.76098587, -0.62245591],\n",
       "       [-0.61517874,  0.5147624 , -0.59713884],\n",
       "       [-0.33867074, -0.80966534, -0.47931635],\n",
       "       [-0.52629529, -0.78190408,  0.33412466]])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'naiveSoftmaxLossAndGradient' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-301-59dde5696626>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m def word2vec_sgd_wrapper(word2vecModel, word2Ind, wordVectors, dataset,\n\u001b[1;32m      2\u001b[0m                          \u001b[0mwindowSize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                          word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mbatchsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'naiveSoftmaxLossAndGradient' is not defined"
     ]
    }
   ],
   "source": [
    "def word2vec_sgd_wrapper(word2vecModel, word2Ind, wordVectors, dataset,\n",
    "                         windowSize,\n",
    "                         word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
    "    batchsize = 50\n",
    "    loss = 0.0\n",
    "    grad = np.zeros(wordVectors.shape)\n",
    "    N = wordVectors.shape[0]\n",
    "    centerWordVectors = wordVectors[:int(N/2),:]\n",
    "    outsideVectors = wordVectors[int(N/2):,:]\n",
    "    for i in range(batchsize):\n",
    "        windowSize1 = random.randint(1, windowSize)\n",
    "        centerWord, context = dataset.getRandomContext(windowSize1)\n",
    "\n",
    "        c, gin, gout = word2vecModel(\n",
    "            centerWord, windowSize1, context, word2Ind, centerWordVectors,\n",
    "            outsideVectors, dataset, word2vecLossAndGradient\n",
    "        )\n",
    "        loss += c / batchsize\n",
    "        grad[:int(N/2), :] += gin / batchsize\n",
    "        grad[int(N/2):, :] += gout / batchsize\n",
    "\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3, -2, -1,  0,  1,  2,  3])"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(-3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.sampleTokenIdx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 0, 1, 4, 0, 0, 4, 4, 3, 1]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K=10\n",
    "negSampleWordIndices = [None] * K\n",
    "for k in range(K):\n",
    "    newidx = dataset.sampleTokenIdx()\n",
    "    while newidx == outsideWordIdx:\n",
    "        newidx = dataset.sampleTokenIdx()\n",
    "    negSampleWordIndices[k] = newidx\n",
    "\n",
    "negSampleWordIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [1] + negSampleWordIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.33867074, -0.80966534, -0.47931635],\n",
       "       [-0.6831809 , -0.04200519,  0.72904007],\n",
       "       [ 0.18289107,  0.76098587, -0.62245591],\n",
       "       [-0.52629529, -0.78190408,  0.33412466],\n",
       "       [-0.6831809 , -0.04200519,  0.72904007],\n",
       "       [-0.6831809 , -0.04200519,  0.72904007],\n",
       "       [-0.52629529, -0.78190408,  0.33412466],\n",
       "       [-0.52629529, -0.78190408,  0.33412466],\n",
       "       [-0.33867074, -0.80966534, -0.47931635],\n",
       "       [ 0.18289107,  0.76098587, -0.62245591]])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outsideVectors[negSampleWordIndices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices: [1, 0, 4, 4, 4, 1, 4, 3, 4, 4, 1]\n",
      "\n",
      "\n",
      "negOutsideVecs: [[ 0.18289107  0.76098587 -0.62245591]\n",
      " [-0.6831809  -0.04200519  0.72904007]\n",
      " [-0.52629529 -0.78190408  0.33412466]\n",
      " [-0.52629529 -0.78190408  0.33412466]\n",
      " [-0.52629529 -0.78190408  0.33412466]\n",
      " [ 0.18289107  0.76098587 -0.62245591]\n",
      " [-0.52629529 -0.78190408  0.33412466]\n",
      " [-0.33867074 -0.80966534 -0.47931635]\n",
      " [-0.52629529 -0.78190408  0.33412466]\n",
      " [-0.52629529 -0.78190408  0.33412466]\n",
      " [ 0.18289107  0.76098587 -0.62245591]]\n",
      "\n",
      "\n",
      "sigmoid(np.dot(negOutsideVecs, centerWordVec)): [0.36627719 0.70614176 0.59011378 0.59011378 0.59011378 0.36627719\n",
      " 0.59011378 0.38553435 0.59011378 0.59011378 0.36627719]\n",
      "\n",
      "\n",
      "centerWordVec: [-0.27323645  0.12538062  0.95374082]\n",
      "Loss: 6.147756674683202\n",
      "\n",
      "\n",
      "gradCenterVec: [-0.99656425 -1.28637278 -0.09357304]\n",
      "\n",
      "\n",
      "gradOutsideVecs: [[ 0.08029278 -0.03684413 -0.28026459]\n",
      " [ 0.17315617 -0.07945656 -0.60440731]\n",
      " [ 0.15941535 -0.07315128 -0.55644454]\n",
      " [ 0.16789441 -0.07704209 -0.58604097]\n",
      " [ 0.11199585 -0.05139179 -0.39092522]]\n"
     ]
    }
   ],
   "source": [
    "K=10\n",
    "negSampleWordIndices = [None] * K\n",
    "for k in range(K):\n",
    "    newidx = dataset.sampleTokenIdx()\n",
    "    while newidx == outsideWordIdx:\n",
    "        newidx = dataset.sampleTokenIdx()\n",
    "    negSampleWordIndices[k] = newidx\n",
    "\n",
    "indices = [1] + negSampleWordIndices\n",
    "print(\"indices: {}\".format(indices))\n",
    "print(\"\\n\")\n",
    "\n",
    "negOutsideVecs = outsideVectors[indices, :] #u_k\n",
    "print(\"negOutsideVecs: {}\".format(negOutsideVecs))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Loss\n",
    "print(\"sigmoid(np.dot(negOutsideVecs, centerWordVec)): {}\".format(sigmoid(np.dot(negOutsideVecs, centerWordVec))))\n",
    "print(\"\\n\")\n",
    "print(\"centerWordVec: {}\".format(centerWordVec))\n",
    "\n",
    "loss = sigmoid(np.dot(u_o, centerWordVec))+np.sum(sigmoid(np.dot(negOutsideVecs, centerWordVec)))\n",
    "print(\"Loss: {}\".format(loss))\n",
    "print(\"\\n\")\n",
    "\n",
    "# dJ / dv_c\n",
    "gradCenterVec = u_o * (sigmoid(np.dot(u_o, centerWordVec))-1) - np.sum(negOutsideVecs.T * (sigmoid(np.dot(negOutsideVecs, centerWordVec)) - 1), axis=1)\n",
    "print(\"gradCenterVec: {}\".format(gradCenterVec))\n",
    "print(\"\\n\")\n",
    "\n",
    "# dJ / du_k\n",
    "gradOutsideVecs = np.zeros(outsideVectors.shape)\n",
    "\n",
    "dJ_du_o = (sigmoid(np.dot(u_o, centerWordVec)) - 1) * centerWordVec\n",
    "\n",
    "dsigmoid_u = sigmoid(np.dot(negOutsideVecs, centerWordVec)) - 1\n",
    "dJ_du_k = np.dot(dsigmoid_u[:, None], centerWordVec[None, :])\n",
    "\n",
    "gradOutsideVecs[outsideWordIdx, :] = dJ_du_o\n",
    "gradOutsideVecs[indices, :] = dJ_du_k\n",
    "print(\"gradOutsideVecs: {}\".format(gradOutsideVecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outside Vectors: \n",
      " [[-0.6831809  -0.04200519  0.72904007]\n",
      " [ 0.18289107  0.76098587 -0.62245591]\n",
      " [-0.61517874  0.5147624  -0.59713884]\n",
      " [-0.33867074 -0.80966534 -0.47931635]\n",
      " [-0.52629529 -0.78190408  0.33412466]]\n",
      "Center Vectors: \n",
      " [[-0.96735714 -0.02182641  0.25247529]\n",
      " [ 0.73663029 -0.48088687 -0.47552459]\n",
      " [-0.27323645  0.12538062  0.95374082]\n",
      " [-0.56713774 -0.27178229 -0.77748902]\n",
      " [-0.59609459  0.7795666   0.19221644]]\n",
      "Sampled indices: \n",
      " [0, 1, 4, 0, 4, 4, 3, 0, 4, 0]\n"
     ]
    }
   ],
   "source": [
    "outsideWordIdx = 2\n",
    "centerWordIdx = 2\n",
    "\n",
    "print(\"Outside Vectors: \\n {}\".format(outsideVectors))\n",
    "print(\"Center Vectors: \\n {}\".format(centerWordVectors))\n",
    "\n",
    "\n",
    "K=10\n",
    "negSampleWordIndices = [None] * K\n",
    "for k in range(K):\n",
    "    newidx = dataset.sampleTokenIdx()\n",
    "    while newidx == outsideWordIdx:\n",
    "        newidx = dataset.sampleTokenIdx()\n",
    "    negSampleWordIndices[k] = newidx\n",
    "    \n",
    "indices = negSampleWordIndices\n",
    "centerWordVec = centerWordVectors[centerWordIdx, :]\n",
    "\n",
    "print(\"Sampled indices: \\n {}\".format(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.61517874  0.5147624  -0.59713884]\n",
      "[[-0.6831809  -0.04200519  0.72904007]\n",
      " [ 0.18289107  0.76098587 -0.62245591]\n",
      " [-0.52629529 -0.78190408  0.33412466]\n",
      " [-0.6831809  -0.04200519  0.72904007]\n",
      " [-0.52629529 -0.78190408  0.33412466]\n",
      " [-0.52629529 -0.78190408  0.33412466]\n",
      " [-0.33867074 -0.80966534 -0.47931635]\n",
      " [-0.6831809  -0.04200519  0.72904007]\n",
      " [-0.52629529 -0.78190408  0.33412466]\n",
      " [-0.6831809  -0.04200519  0.72904007]]\n",
      "[-0.27323645  0.12538062  0.95374082]\n"
     ]
    }
   ],
   "source": [
    "u_o = outsideVectors[outsideWordIdx, :]\n",
    "print(u_o)\n",
    "\n",
    "u_k_matrix = outsideVectors[indices, :]\n",
    "print(u_k_matrix)\n",
    "\n",
    "print(centerWordVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.35891601,  0.30032973, -0.34839093])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_o*(1-sigmoid(np.dot(centerWordVec, u_o)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.70614176, 0.36627719, 0.59011378, 0.70614176, 0.59011378,\n",
       "       0.59011378, 0.38553435, 0.70614176, 0.59011378, 0.70614176])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = 1-sigmoid(-np.dot(u_k_matrix, centerWordVec))\n",
    "a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.48242257, -0.02966162,  0.51480564],\n",
       "       [ 0.06698883,  0.27873177, -0.22799141],\n",
       "       [-0.3105741 , -0.46141237,  0.19717156],\n",
       "       [-0.48242257, -0.02966162,  0.51480564],\n",
       "       [-0.3105741 , -0.46141237,  0.19717156],\n",
       "       [-0.3105741 , -0.46141237,  0.19717156],\n",
       "       [-0.1305692 , -0.3121538 , -0.18479292],\n",
       "       [-0.48242257, -0.02966162,  0.51480564],\n",
       "       [-0.3105741 , -0.46141237,  0.19717156],\n",
       "       [-0.48242257, -0.02966162,  0.51480564]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_k_matrix * a1[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.23556705, -1.99771799,  2.43512448])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(u_k_matrix * a1[:, None], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.409279830513372"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.log(sigmoid(-np.dot(u_k_matrix, centerWordVec))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices: \n",
      "[0, 1, 4, 0, 4, 4, 3, 0, 4, 0]\n",
      "Outside Vectors: \n",
      "[[-0.6831809  -0.04200519  0.72904007]\n",
      " [ 0.18289107  0.76098587 -0.62245591]\n",
      " [-0.61517874  0.5147624  -0.59713884]\n",
      " [-0.33867074 -0.80966534 -0.47931635]\n",
      " [-0.52629529 -0.78190408  0.33412466]]\n",
      "Center Vector: \n",
      "[-0.27323645  0.12538062  0.95374082]\n",
      "U_o: \n",
      "[-0.61517874  0.5147624  -0.59713884]\n",
      "U_k matrix: \n",
      "[[-0.6831809  -0.04200519  0.72904007]\n",
      " [ 0.18289107  0.76098587 -0.62245591]\n",
      " [-0.52629529 -0.78190408  0.33412466]\n",
      " [-0.6831809  -0.04200519  0.72904007]\n",
      " [-0.52629529 -0.78190408  0.33412466]\n",
      " [-0.52629529 -0.78190408  0.33412466]\n",
      " [-0.33867074 -0.80966534 -0.47931635]\n",
      " [-0.6831809  -0.04200519  0.72904007]\n",
      " [-0.52629529 -0.78190408  0.33412466]\n",
      " [-0.6831809  -0.04200519  0.72904007]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Indices: \\n{}\".format(indices))\n",
    "print(\"Outside Vectors: \\n{}\".format(outsideVectors))\n",
    "print(\"Center Vector: \\n{}\".format(centerWordVec))\n",
    "print(\"U_o: \\n{}\".format(u_o))\n",
    "print(\"U_k matrix: \\n{}\".format(u_k_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.15941535,  0.07315128,  0.55644454])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-sigmoid(np.dot(u_o, centerWordVec)))*centerWordVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.19294367 -0.08853649 -0.67347622]\n",
      "[ 0.10008028 -0.04592406 -0.34933351]\n",
      "[ 0.16124059 -0.07398883 -0.5628156 ]\n",
      "[ 0.19294367 -0.08853649 -0.67347622]\n",
      "[ 0.16124059 -0.07398883 -0.5628156 ]\n",
      "[ 0.16124059 -0.07398883 -0.5628156 ]\n",
      "[ 0.10534204 -0.04833854 -0.36769985]\n",
      "[ 0.19294367 -0.08853649 -0.67347622]\n",
      "[ 0.16124059 -0.07398883 -0.5628156 ]\n",
      "[ 0.19294367 -0.08853649 -0.67347622]\n"
     ]
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    print((sigmoid(-np.dot(u_k_matrix[k, :], centerWordVec))-1)*centerWordVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    orig_shape = x.shape\n",
    "\n",
    "    if len(x.shape) > 1:\n",
    "        # Matrix\n",
    "        tmp = np.max(x, axis=1)\n",
    "        x -= tmp.reshape((x.shape[0], 1))\n",
    "        x = np.exp(x)\n",
    "        tmp = np.sum(x, axis=1)\n",
    "        x /= tmp.reshape((x.shape[0], 1))\n",
    "    else:\n",
    "        # Vector\n",
    "        tmp = np.max(x)\n",
    "        x -= tmp\n",
    "        x = np.exp(x)\n",
    "        tmp = np.sum(x)\n",
    "        x /= tmp\n",
    "\n",
    "    assert x.shape == orig_shape\n",
    "    return x\n",
    "\n",
    "def naiveSoftmaxLossAndGradient(\n",
    "    centerWordVec,\n",
    "    outsideWordIdx,\n",
    "    outsideVectors,\n",
    "    dataset\n",
    "):\n",
    "\n",
    "    # Compute 'y'\n",
    "    num_words = outsideVectors.shape[0]\n",
    "\n",
    "    y = np.zeros(num_words)\n",
    "    y[outsideWordIdx] = 1\n",
    "\n",
    "    # Compute 'y hat' by getting the U transpose * v_c for every outside word\n",
    "    denominator = np.dot(outsideVectors, centerWordVec)\n",
    "    y_hat = softmax(denominator)\n",
    "\n",
    "    # Calculate loss for the mentioned outside word\n",
    "    loss = -np.log(y_hat[outsideWordIdx])\n",
    "\n",
    "    # Calculate Gradient wrt center word\n",
    "    gradCenterVec = np.dot(outsideVectors.T, y-y_hat)\n",
    "\n",
    "    # Calculate Gradient wrt all outside word\n",
    "    gradOutsideVecs = np.dot((y-y_hat)[:, None], centerWordVec[None, :])\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return loss, gradCenterVec, gradOutsideVecs\n",
    "\n",
    "def getNegativeSamples(outsideWordIdx, dataset, K):\n",
    "    \"\"\" Samples K indexes which are not the outsideWordIdx \"\"\"\n",
    "\n",
    "    negSampleWordIndices = [None] * K\n",
    "    for k in range(K):\n",
    "        newidx = dataset.sampleTokenIdx()\n",
    "        while newidx == outsideWordIdx:\n",
    "            newidx = dataset.sampleTokenIdx()\n",
    "        negSampleWordIndices[k] = newidx\n",
    "    return negSampleWordIndices\n",
    "\n",
    "\n",
    "def negSamplingLossAndGradient(\n",
    "    centerWordVec,\n",
    "    outsideWordIdx,\n",
    "    outsideVectors,\n",
    "    dataset,\n",
    "    K=10\n",
    "):\n",
    "\n",
    "    negSampleWordIndices = getNegativeSamples(outsideWordIdx, dataset, K)\n",
    "    indices = [outsideWordIdx] + negSampleWordIndices\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### Please use your implementation of sigmoid in here.\n",
    "    u_o = outsideVectors[outsideWordIdx, :]\n",
    "\n",
    "    negOutsideVecs = outsideVectors[negSampleWordIndices, :] #u_k\n",
    "\n",
    "    # Loss\n",
    "    loss = np.log(sigmoid(np.dot(u_o, centerWordVec))) + np.sum(np.log(sigmoid(-np.dot(negOutsideVecs, centerWordVec))))\n",
    "    #loss = np.sum(sigmoid(np.dot(negOutsideVecs, centerWordVec)))\n",
    "\n",
    "    # dJ / dv_c\n",
    "    gradCenterVec_t1 = u_o*(sigmoid(np.dot(centerWordVec, u_o))-1)\n",
    "\n",
    "    gradCenterVec_t2_1 = sigmoid(-np.dot(negOutsideVecs, centerWordVec))-1\n",
    "    gradCenterVec_t2 = np.sum(negOutsideVecs * gradCenterVec_t2_1[:, None], axis=0)\n",
    "\n",
    "    gradCenterVec =  gradCenterVec_t1 - gradCenterVec_t2\n",
    "\n",
    "    #gradCenterVec = u_o * (sigmoid(np.dot(u_o, centerWordVec))-1) - np.sum(negOutsideVecs.T * (sigmoid(np.dot(negOutsideVecs, centerWordVec)) - 1), axis=1)\n",
    "\n",
    "    # dJ / du_k\n",
    "    gradOutsideVecs = np.zeros(outsideVectors.shape)\n",
    "\n",
    "    gradOutsideVecs[outsideWordIdx, :] += centerWordVec * (sigmoid(np.dot(centerWordVec, u_o))-1)\n",
    "\n",
    "    for k in range(len(negSampleWordIndices)):\n",
    "        negSampleWordIndex = negSampleWordIndices[k]\n",
    "        u_k = outsideVectors[negSampleWordIndex, :]\n",
    "        gradOutsideVecs[negSampleWordIndex, :] += centerWordVec * (1- sigmoid(-np.dot(centerWordVec, u_k)))\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return loss, gradCenterVec, gradOutsideVecs\n",
    "\n",
    "\n",
    "def skipgram(currentCenterWord, windowSize, outsideWords, word2Ind,\n",
    "             centerWordVectors, outsideVectors, dataset,\n",
    "             word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
    "\n",
    "    loss = 0.0\n",
    "    gradCenterVecs = np.zeros(centerWordVectors.shape)\n",
    "    gradOutsideVectors = np.zeros(outsideVectors.shape)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    centerWordIdx = word2Ind[currentCenterWord]\n",
    "    centerWordVec = centerWordVectors[centerWordIdx, :]\n",
    "\n",
    "    for j in np.arange(-windowSize, windowSize+1):\n",
    "        if j != 0 and centerWordIdx + j < outsideVectors.shape[0]:\n",
    "            outsideWordIdx = centerWordIdx + j\n",
    "            temp_loss,temp_gradCenterVecs,temp_gradOutsideVecs = word2vecLossAndGradient(\n",
    "            centerWordVec = centerWordVec,\n",
    "            outsideWordIdx = outsideWordIdx,\n",
    "            outsideVectors = outsideVectors,\n",
    "            dataset = dataset)\n",
    "\n",
    "            loss -= temp_loss\n",
    "            gradCenterVecs[centerWordIdx, :] += temp_gradCenterVecs\n",
    "            gradOutsideVectors += temp_gradOutsideVecs\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return loss, gradCenterVecs, gradOutsideVectors\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \n",
    "    s = 1/(1+np.exp(-x))\n",
    "\n",
    "    return s\n",
    "\n",
    "def normalizeRows(x):\n",
    "    N = x.shape[0]\n",
    "    x /= np.sqrt(np.sum(x**2, axis=1)).reshape((N,1)) + 1e-30\n",
    "    return x\n",
    "\n",
    "import random\n",
    "\n",
    "dataset = type('dummy', (), {})()\n",
    "def dummySampleTokenIdx():\n",
    "    return random.randint(0, 4)\n",
    "\n",
    "def getRandomContext(C):\n",
    "    tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "    return tokens[random.randint(0,4)], \\\n",
    "        [tokens[random.randint(0,4)] for i in range(2*C)]\n",
    "dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "dataset.getRandomContext = getRandomContext\n",
    "\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "dummy_vectors = normalizeRows(np.random.randn(10,3))\n",
    "dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.217424877675181,\n",
      " Center Vector Grads: \n",
      "[ 0.17249875 -0.64873662 -0.67821423],\n",
      " Outside Vector Gradient: \n",
      "[[ 0.11394933 -0.05228819 -0.39774391]\n",
      " [ 0.02740743 -0.01257651 -0.09566654]\n",
      " [ 0.03385715 -0.01553611 -0.11817949]\n",
      " [-0.24348396  0.11172803  0.84988879]\n",
      " [ 0.06827005 -0.03132723 -0.23829885]]\n",
      "\n",
      "\n",
      "Loss: 1.386866395233244,\n",
      " Center Vector Grads: \n",
      "[-0.0151258  -0.62097535  0.13522678],\n",
      " Outside Vector Gradient: \n",
      "[[ 0.11394933 -0.05228819 -0.39774391]\n",
      " [ 0.02740743 -0.01257651 -0.09566654]\n",
      " [ 0.03385715 -0.01553611 -0.11817949]\n",
      " [ 0.02975248 -0.01365259 -0.10385203]\n",
      " [-0.2049664   0.09405339  0.71544197]]\n",
      "\n",
      "\n",
      "Loss: 0.8745836004406865,\n",
      " Center Vector Grads: \n",
      "[-0.17201142  0.11892354  0.53014219],\n",
      " Outside Vector Gradient: \n",
      "[[-0.15928711  0.07309244  0.55599691]\n",
      " [ 0.02740743 -0.01257651 -0.09566654]\n",
      " [ 0.03385715 -0.01553611 -0.11817949]\n",
      " [ 0.02975248 -0.01365259 -0.10385203]\n",
      " [ 0.06827005 -0.03132723 -0.23829885]]\n",
      "\n",
      "\n",
      "Loss: 2.299523385496988,\n",
      " Center Vector Grads: \n",
      "[ 0.69406055  0.9219146  -0.82135379],\n",
      " Outside Vector Gradient: \n",
      "[[ 0.11394933 -0.05228819 -0.39774391]\n",
      " [-0.24582902  0.11280411  0.85807428]\n",
      " [ 0.03385715 -0.01553611 -0.11817949]\n",
      " [ 0.02975248 -0.01365259 -0.10385203]\n",
      " [ 0.06827005 -0.03132723 -0.23829885]]\n",
      "\n",
      "\n",
      "Loss: 2.217424877675181,\n",
      " Center Vector Grads: \n",
      "[ 0.17249875 -0.64873662 -0.67821423],\n",
      " Outside Vector Gradient: \n",
      "[[ 0.11394933 -0.05228819 -0.39774391]\n",
      " [ 0.02740743 -0.01257651 -0.09566654]\n",
      " [ 0.03385715 -0.01553611 -0.11817949]\n",
      " [-0.24348396  0.11172803  0.84988879]\n",
      " [ 0.06827005 -0.03132723 -0.23829885]]\n",
      "\n",
      "\n",
      "Loss: 1.386866395233244,\n",
      " Center Vector Grads: \n",
      "[-0.0151258  -0.62097535  0.13522678],\n",
      " Outside Vector Gradient: \n",
      "[[ 0.11394933 -0.05228819 -0.39774391]\n",
      " [ 0.02740743 -0.01257651 -0.09566654]\n",
      " [ 0.03385715 -0.01553611 -0.11817949]\n",
      " [ 0.02975248 -0.01365259 -0.10385203]\n",
      " [-0.2049664   0.09405339  0.71544197]]\n",
      "\n",
      "\n",
      "Loss: 10.382689531754526\n",
      "Gradient wrt Center Vectors (dJ/dV):\n",
      " [[ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.83679504 -1.49858579 -1.37718651]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      "Gradient wrt Outside Vectors (dJ/dU):\n",
      " [[ 0.41045956 -0.18834851 -1.43272264]\n",
      " [-0.10879187  0.04992157  0.3797416 ]\n",
      " [ 0.20314289 -0.09321664 -0.70907696]\n",
      " [-0.36795798  0.16884571  1.28436947]\n",
      " [-0.13685261  0.06279786  0.47768853]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def skipgram(currentCenterWord, windowSize, outsideWords, word2Ind,\n",
    "             centerWordVectors, outsideVectors, dataset,\n",
    "             word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
    "\n",
    "    loss = 0.0\n",
    "    gradCenterVecs = np.zeros(centerWordVectors.shape)\n",
    "    gradOutsideVectors = np.zeros(outsideVectors.shape)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    centerWordIdx = word2Ind[currentCenterWord]\n",
    "    centerWordVec = centerWordVectors[centerWordIdx, :]\n",
    "\n",
    "    for j in np.arange(-windowSize-1, windowSize):\n",
    "        if j != 0 and centerWordIdx + j < outsideVectors.shape[0]:\n",
    "            outsideWordIdx = centerWordIdx + j\n",
    "            temp_loss,temp_gradCenterVecs,temp_gradOutsideVecs = word2vecLossAndGradient(\n",
    "            centerWordVec = centerWordVec,\n",
    "            outsideWordIdx = outsideWordIdx,\n",
    "            outsideVectors = outsideVectors,\n",
    "            dataset = dataset)\n",
    "            \n",
    "            print(\"Loss: {},\\n Center Vector Grads: \\n{},\\n Outside Vector Gradient: \\n{}\".format(temp_loss, temp_gradCenterVecs, temp_gradOutsideVecs))\n",
    "            print(\"\\n\")\n",
    "            loss += temp_loss\n",
    "            gradCenterVecs[centerWordIdx, :] += temp_gradCenterVecs\n",
    "            gradOutsideVectors += temp_gradOutsideVecs\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return loss, gradCenterVecs, gradOutsideVectors\n",
    "\n",
    "\n",
    "print(\"Loss: {}\\nGradient wrt Center Vectors (dJ/dV):\\n {}\\nGradient wrt Outside Vectors (dJ/dU):\\n {}\\n\".format(\n",
    "            *skipgram(\"c\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"],\n",
    "                dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset)\n",
    "        )\n",
    "    )\n",
    "# print(\"Loss: {}\\nGradient wrt Center Vectors (dJ/dV):\\n {}\\nGradient wrt Outside Vectors (dJ/dU):\\n {}\\n\".format(\n",
    "#             *skipgram(currentCenterWord=\"c\",\n",
    "#                       windowSize=3,\n",
    "#                       outsideWords=[\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"],\n",
    "#                       word2Ind=dummy_tokens,\n",
    "#                       centerWordVectors=dummy_vectors[:5,:],\n",
    "#                       outsideVectors=dummy_vectors[5:,:], dataset)))\n",
    "\n",
    "#[-1.26947339 -1.36873189  2.45158957]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -6.474509209033591,\n",
      " Center Vector Grads: \n",
      "[-1.50121937 -1.32202608 -1.13057713],\n",
      " Outside Vector Gradient: \n",
      "[[ 0.08029278 -0.03684413 -0.28026459]\n",
      " [-0.20016056  0.09184813  0.69866702]\n",
      " [-0.22764219  0.10445868  0.79459256]\n",
      " [-0.31602611  0.14501561  1.10309954]\n",
      " [-0.48372177  0.2219665   1.68844679]]\n",
      "\n",
      "\n",
      "Loss: -9.000745319148063,\n",
      " Center Vector Grads: \n",
      "[-3.08855121 -2.00165977  1.28134996],\n",
      " Outside Vector Gradient: \n",
      "[[-0.578831    0.26560948  2.02042866]\n",
      " [ 0.17315617 -0.07945656 -0.60440731]\n",
      " [-0.22764219  0.10445868  0.79459256]\n",
      " [-0.31602611  0.14501561  1.10309954]\n",
      " [-0.32248118  0.14797767  1.1256312 ]]\n",
      "\n",
      "\n",
      "Loss: -15.475254528181654\n",
      "Gradient wrt Center Vectors (dJ/dV):\n",
      " [[ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [-4.58977058 -3.32368585  0.15077283]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      " Gradient wrt Outside Vectors (dJ/dU):\n",
      " [[-0.49853822  0.22876535  1.74016407]\n",
      " [-0.02700439  0.01239157  0.09425972]\n",
      " [-0.45528438  0.20891737  1.58918512]\n",
      " [-0.63205221  0.29003122  2.20619908]\n",
      " [-0.80620296  0.36994417  2.81407799]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss: {}\\nGradient wrt Center Vectors (dJ/dV):\\n {}\\n Gradient wrt Outside Vectors (dJ/dU):\\n {}\\n\".format(\n",
    "        *skipgram(\"c\", 1, [\"a\", \"b\"], dummy_tokens, dummy_vectors[:5,:],\n",
    "            dummy_vectors[5:,:], dataset, negSamplingLossAndGradient)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
